{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics with SparkML\n",
    "#### pyspark.ml.stat is only available in Spark 2.4.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master('local[5]').appName('SPark ML Statistics Handson').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    spark\n",
    "    .read\n",
    "    .csv(\n",
    "        'H:\\Training\\PySpark\\data\\yellow_tripdata_2019-01.csv_Pieces/yellow_tripdata_2019-01_1.csv'\n",
    "        , header=True\n",
    "        , inferSchema=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['VendorID', 'tpep_pickup_datetime', 'tpep_dropoff_datetime', 'passenger_count', 'trip_distance', 'RatecodeID', 'store_and_fwd_flag', 'PULocationID', 'DOLocationID', 'payment_type', 'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'improvement_surcharge', 'total_amount', 'congestion_surcharge']\n"
     ]
    }
   ],
   "source": [
    "print(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfselect=df.select(cols[3:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfRDD=dfselect.rdd.map(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[8, 5.08],\n",
       " [8, 0.01],\n",
       " [6, 40.77],\n",
       " [6, 23.77],\n",
       " [6, 22.96],\n",
       " [6, 21.39],\n",
       " [6, 20.25],\n",
       " [6, 20.11],\n",
       " [6, 19.96],\n",
       " [6, 19.8]]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfRDD.top(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.stat import Statistics\n",
    "summary = Statistics.colStats(dfRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.71366  , 3.0834406])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.71366   3.0834406]\n",
      "[ 1.64426229 10.19566132]\n",
      "[49344. 49545.]\n"
     ]
    }
   ],
   "source": [
    "print(summary.mean())  # a dense vector containing the mean value for each column\n",
    "print(summary.variance())  # column-wise variance\n",
    "print(summary.numNonzeros())  # number of nonzeros in each column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.000000e+00 -2.789746e-04]\n",
      " [-2.789746e-04  1.000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "print(Statistics.corr(dfRDD, method=\"pearson\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis Testing\n",
    "#### Statistics provides methods to run Pearsonâ€™s chi-squared tests. The following example demonstrates how to run and interpret hypothesis tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------+\n",
      "|passenger_count|trip_distance|\n",
      "+---------------+-------------+\n",
      "|              1|          1.5|\n",
      "|              1|          2.6|\n",
      "|              3|          0.0|\n",
      "|              5|          0.0|\n",
      "|              5|          0.0|\n",
      "|              5|          0.0|\n",
      "|              5|          0.0|\n",
      "|              1|          1.3|\n",
      "|              1|          3.7|\n",
      "|              2|          2.1|\n",
      "|              2|          2.8|\n",
      "|              1|          0.7|\n",
      "|              1|          8.7|\n",
      "|              1|          6.3|\n",
      "|              1|          2.7|\n",
      "|              1|         0.38|\n",
      "|              1|         0.55|\n",
      "|              1|          0.3|\n",
      "|              1|         1.42|\n",
      "|              1|         1.72|\n",
      "+---------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfselect.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf=dfselect.crosstab('passenger_count','trip_distance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm= np.array(cf.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "independenceTestResult = Statistics.chiSqTest(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chi squared test summary:\n",
      "method: pearson\n",
      "degrees of freedom = 14135 \n",
      "statistic = 1261600.1704404952 \n",
      "pValue = 0.0 \n",
      "Very strong presumption against null hypothesis: observed follows the same distribution as expected..\n"
     ]
    }
   ],
   "source": [
    "print(independenceTestResult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chi squared test summary:\n",
      "method: pearson\n",
      "degrees of freedom = 4 \n",
      "statistic = 0.12499999999999999 \n",
      "pValue = 0.998126379239318 \n",
      "No presumption against null hypothesis: observed follows the same distribution as expected..\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vec = Vectors.dense(0.1, 0.15, 0.2, 0.3, 0.25)  # a vector composed of the frequencies of events\n",
    "\n",
    "# compute the goodness of fit. If a second vector to test against\n",
    "# is not supplied as a parameter, the test runs against a uniform distribution.\n",
    "goodnessOfFitTestResult = Statistics.chiSqTest(vec)\n",
    "\n",
    "# summary of the test including the p-value, degrees of freedom,\n",
    "# test statistic, the method used, and the null hypothesis.\n",
    "print(\"%s\\n\" % goodnessOfFitTestResult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    " from pyspark.mllib.stat import KernelDensity\n",
    "\n",
    "# an RDD of sample data\n",
    "data = sc.parallelize([1.0, 1.0, 1.0, 2.0, 3.0, 4.0, 5.0, 5.0, 6.0, 7.0, 8.0, 9.0, 9.0])\n",
    "\n",
    "# Construct the density estimator with the sample data and a standard deviation for the Gaussian\n",
    "# kernels\n",
    "kd = KernelDensity()\n",
    "kd.setSample(data)\n",
    "kd.setBandwidth(3.0)\n",
    "\n",
    "# Find density estimates for the given values\n",
    "densities = kd.estimate([-1.0, 2.0, 5.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'memoryview' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-130-1a5af6efece3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdensities\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'memoryview' object is not callable"
     ]
    }
   ],
   "source": [
    "densities.data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
